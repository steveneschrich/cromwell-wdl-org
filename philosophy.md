# Philosophy for Cromwell/WDL

Not an existential discussion, but rather some principles that I found useful when thinking about developing/using these pipelines.

## Separating Bioinformatics and Engineering
The only time I focus on the engineering aspects of cromwell/wdl is when I'm trying to solve a very specific problem. This is always dangerous because (a) I'm supposed to be doing something else and (b) I'm not designing for general-purpose use. I think many of the solutions that I've developed are with very particular things in mind and sometimes conflict with a more general approach.

So to the extent possible, trying to decide what paired-read trimming algorithm to use when you are developing a way to handle reference volumes is not the right move. Either develop within the context of your current structure to solve a specific problem or day-dream about approaches when there is nothing else going. It is hard to do both simultaneously.

## Isolation vs. project directory
This is a big topic that took a long time to evolve. The purpose behind cromwell is to have a (mostly) isolated execution environment such that results are independent and reproducible without concern to external conditions. That is a wonderful goal. However, in practice I have found that it really can't work as designed for me. Some of my concerns:

- many, many symlink/hardlinks to files that could be relevant but buried deep within `cromwell-executions`.
- needing to copy files out of the very deep `cromwell-executions` before removing and before analyzing
- duplicate output files may get produced, or multiple runs may produce the same files again

Given the way I work, I consider a "project" directory to be the unit of analysis. That is, raw data is imported into the project, processed in a variety of ways, then reported on. And using R, etc I have things like a `data-raw` directory, a `data` directory (derived data), a `reports` directory, etc. To me, these are the fundamental stores that I would want a pipeline to operate on. Go ahead and overwrite files in the `data` directory again and again; if the pipeline is reproducible it is just overwriting the same thing. When I shifted to thinking of a pipeline as manipulating a project directory in various ways to produce output files and reports, the coding and work became much simpler. Before that, I was trying to pass various things around in tasks and workflows, keeping track of many different files (or arrays of file) so that the final workflow can spit out a (very long) list of files. Which is great from a reprodicibility and isolation sense, but is very difficult to develop for. Rather, the workflow produces output files and/or reports in a consistent directory hierarchy within the project directory. In this type of thinking, the workflows are just highly complex, highly bundled executables running jobs on files within the project directory and producing files within the project directory. Just like a native R script does. 

Note that this viewpoint has implications on the hard link vs. soft link type of argument. If our true goal is to create reproducible pipelines then engineering the pipeline code to be reproducible while referencing data within a project does not seem a large exception. Localization, while important in a cloud context, is less important to managing a directory state with well-written code.

## Piece-wise vs all-in-one
This is more of a scientific philosophy and a development approach. Too often I've experienced the rather haphazard, chaotic research experience. Despite our best planning and intentions, we always assess results as we go and make mid-course corrections. To do otherwise would be very time-consuming and resource wasteful. While my original, naive view of pipelines was that there was "one pipeline to rule them all", in fact my experiences have taught me that there are many steps which are themselves pipelines. When I developed an rnaseq pipeline, I ended up with a pre-align qc, cleaning, alignment, quantification pipelines. Each had it's own challenges and rearrangment lifecycle. Even trying to imagine wiring up the entire pipeline when I started was impossible. There are too many decisions that get made along the way with implications. Therefore, for me, it is much easier to develop a component out, develop the next component out, then when I'm done consider wrapping these into a single pipeline. Like science in general, being too near-sighted or far-sighted can cause problems - developing piecewise with a bigger picture in mind has always made more sense to me.
